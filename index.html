<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Remotely Sensing Cities and Environments</title>
    <meta charset="utf-8" />
    <meta name="author" content="Andy MacLachlan" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies-fonts.css" rel="stylesheet" />
    <script src="libs/js-cookie/js.cookie.js"></script>
    <script src="libs/peerjs/peerjs.min.js"></script>
    <script src="libs/tiny.toast/toast.min.js"></script>
    <link href="libs/xaringanExtra-broadcast/broadcast.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-broadcast/broadcast.js"></script>
    <script src="libs/freezeframe/freezeframe.min.js"></script>
    <script src="libs/xaringanExtra-freezeframe/freezeframe-init.js"></script>
    <script id="xaringanExtra-freezeframe-options" type="application/json">{"selector":"img[src$=\"gif\"]","trigger":"click","overlay":false,"responsive":true,"warnings":true}</script>
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <script src="libs/mark.js/mark.min.js"></script>
    <link href="libs/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":false}) })</script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"xb3f4fd5d69c4a95849497129660d366","expires":1}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-progressBar/progress-bar.js"></script>
    <head>
    <link rel="apple-touch-icon" sizes="180x180" href="assets/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/favicon-16x16.png">
    <link rel="manifest" href="assets/site.webmanifest">
    <link rel="mask-icon" href="assets/safari-pinned-tab.svg" color="#5bbad5">
    </head>
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: center, title-slide, middle

background-image: url("img/CASA_Logo_no_text_trans_17.png")
background-size: cover
background-position: center


&lt;style&gt;
.title-slide .remark-slide-number {
  display: none;
}
&lt;/style&gt;


# Remotely Sensing Cities and Environments

### Lecture 7: Classification The Big Questions (Lecture 6 continued) and Accuracy

### 28/06/2022 (updated: 24/02/2023)

<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M511.6 36.86l-64 415.1c-1.5 9.734-7.375 18.22-15.97 23.05c-4.844 2.719-10.27 4.097-15.68 4.097c-4.188 0-8.319-.8154-12.29-2.472l-122.6-51.1l-50.86 76.29C226.3 508.5 219.8 512 212.8 512C201.3 512 192 502.7 192 491.2v-96.18c0-7.115 2.372-14.03 6.742-19.64L416 96l-293.7 264.3L19.69 317.5C8.438 312.8 .8125 302.2 .0625 289.1s5.469-23.72 16.06-29.77l448-255.1c10.69-6.109 23.88-5.547 34 1.406S513.5 24.72 511.6 36.86z"/></svg> [a.maclachlan@ucl.ac.uk](mailto:a.maclachlan@ucl.ac.uk)
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M459.4 151.7c.325 4.548 .325 9.097 .325 13.65 0 138.7-105.6 298.6-298.6 298.6-59.45 0-114.7-17.22-161.1-47.11 8.447 .974 16.57 1.299 25.34 1.299 49.06 0 94.21-16.57 130.3-44.83-46.13-.975-84.79-31.19-98.11-72.77 6.498 .974 12.99 1.624 19.82 1.624 9.421 0 18.84-1.3 27.61-3.573-48.08-9.747-84.14-51.98-84.14-102.1v-1.299c13.97 7.797 30.21 12.67 47.43 13.32-28.26-18.84-46.78-51.01-46.78-87.39 0-19.49 5.197-37.36 14.29-52.95 51.65 63.67 129.3 105.3 216.4 109.8-1.624-7.797-2.599-15.92-2.599-24.04 0-57.83 46.78-104.9 104.9-104.9 30.21 0 57.5 12.67 76.67 33.14 23.72-4.548 46.46-13.32 66.6-25.34-7.798 24.37-24.37 44.83-46.13 57.83 21.12-2.273 41.58-8.122 60.43-16.24-14.29 20.79-32.16 39.31-52.63 54.25z"/></svg> [andymaclachlan](https://twitter.com/andymaclachlan)
<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> [andrewmaclachlan](https://github.com/andrewmaclachlan)
<svg aria-hidden="true" role="img" viewBox="0 0 384 512" style="height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M168.3 499.2C116.1 435 0 279.4 0 192C0 85.96 85.96 0 192 0C298 0 384 85.96 384 192C384 279.4 267 435 215.7 499.2C203.4 514.5 180.6 514.5 168.3 499.2H168.3zM192 256C227.3 256 256 227.3 256 192C256 156.7 227.3 128 192 128C156.7 128 128 156.7 128 192C128 227.3 156.7 256 192 256z"/></svg> [Centre for Advanced Spatial Analysis, UCL](https://www.ucl.ac.uk/bartlett/casa/)
<svg aria-hidden="true" role="img" viewBox="0 0 384 512" style="height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M88 304H80V256H88C101.3 256 112 266.7 112 280C112 293.3 101.3 304 88 304zM192 256H200C208.8 256 216 263.2 216 272V336C216 344.8 208.8 352 200 352H192V256zM224 0V128C224 145.7 238.3 160 256 160H384V448C384 483.3 355.3 512 320 512H64C28.65 512 0 483.3 0 448V64C0 28.65 28.65 0 64 0H224zM64 224C55.16 224 48 231.2 48 240V368C48 376.8 55.16 384 64 384C72.84 384 80 376.8 80 368V336H88C118.9 336 144 310.9 144 280C144 249.1 118.9 224 88 224H64zM160 368C160 376.8 167.2 384 176 384H200C226.5 384 248 362.5 248 336V272C248 245.5 226.5 224 200 224H176C167.2 224 160 231.2 160 240V368zM288 224C279.2 224 272 231.2 272 240V368C272 376.8 279.2 384 288 384C296.8 384 304 376.8 304 368V320H336C344.8 320 352 312.8 352 304C352 295.2 344.8 288 336 288H304V256H336C344.8 256 352 248.8 352 240C352 231.2 344.8 224 336 224H288zM256 0L384 128H256V0z"/></svg> [PDF presentation](https://github.com/andrewmaclachlan/CASA0023-lecture-7/blob/main/index.pdf)


&lt;a href="https://github.com/andrewmaclachlan" class="github-corner" aria-label="View source on GitHub"&gt;&lt;svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"&gt;&lt;path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"&gt;&lt;/path&gt;&lt;path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"&gt;&lt;/path&gt;&lt;path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;style&gt;.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}&lt;/style&gt;

---

<style>.xe__progress-bar__container {
  bottom:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #0051BA;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>

# How to use the lectures



- Slides are made with [xaringan](https://slides.yihui.org/xaringan/#1)

- <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M500.3 443.7l-119.7-119.7c27.22-40.41 40.65-90.9 33.46-144.7C401.8 87.79 326.8 13.32 235.2 1.723C99.01-15.51-15.51 99.01 1.724 235.2c11.6 91.64 86.08 166.7 177.6 178.9c53.8 7.189 104.3-6.236 144.7-33.46l119.7 119.7c15.62 15.62 40.95 15.62 56.57 0C515.9 484.7 515.9 459.3 500.3 443.7zM79.1 208c0-70.58 57.42-128 128-128s128 57.42 128 128c0 70.58-57.42 128-128 128S79.1 278.6 79.1 208z"/></svg> In the bottom left there is a search tool which will search all content of presentation

- Control + F will also search 

- Press enter to move to the next result 

- <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M421.7 220.3L188.5 453.4L154.6 419.5L158.1 416H112C103.2 416 96 408.8 96 400V353.9L92.51 357.4C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6L137.6 429.6C143.1 427.7 149.8 424.2 154.6 419.5L188.5 453.4C178.1 463.8 165.2 471.5 151.1 475.6L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1C.8198 498.8-1.502 489.7 .976 481.2L36.37 360.9C40.53 346.8 48.16 333.9 58.57 323.5L291.7 90.34L421.7 220.3zM492.7 58.75C517.7 83.74 517.7 124.3 492.7 149.3L444.3 197.7L314.3 67.72L362.7 19.32C387.7-5.678 428.3-5.678 453.3 19.32L492.7 58.75z"/></svg> In the top right let's you draw on the slides, although these aren't saved.

- Pressing the letter `o` (for overview) will allow you to see an overview of the whole presentation and go to a slide

- Alternatively just typing the slide number e.g. 10 on the website will take you to that slide

- Pressing alt+F will fit the slide to the screen, this is useful if you have resized the window and have another open - side by side. 

<div>
<style type="text/css">.xaringan-extra-logo {
width: 50px;
height: 128px;
z-index: 0;
background-image: url(img/casa_logo.jpg);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:2em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>
---
# Lecture outline

.pull-left[

### Part 1: Landcover classification (contiuned)



### Part 2: Accuracy 

]

.pull-right[
&lt;img src="img/satellite.png" width="100%" /&gt;
.small[Source:[Original from the British Library. Digitally enhanced by rawpixel.](https://www.rawpixel.com/image/571789/solar-generator-vintage-style)
]]

---
class: inverse, center, middle

# What do we need (current or historic) landcover data for?

---

class: inverse, center, middle

# Can we just used pre-classified data

---

# Pre-classified data

* GlobeLand30 - 30m for 2000, 2010 and 2020: http://www.globallandcover.com/home_en.html?type=data   

* European Space Agency’s (ESA) Climate Change Initiative (CCI) annual global land cover (300 m) (1992-2015): https://climate.esa.int/en/projects/land-cover/data/

* Dynamic World - near real time 10m: https://www.dynamicworld.app/explore/ 
  * A major benefit of an AI-powered approach is the model looks at an incoming Sentinel-2 satellite image and, for every pixel in the image, estimates the degree of tree cover, how built up a particular area is, or snow coverage if there’s been a recent snowstorm, for example

* MODIS: https://modis.gsfc.nasa.gov/data/dataprod/mod12.php

* Google building data: https://sites.research.google/open-buildings/ 

---

# Dynamic World 

.panelset[

.panel[.panel-name[Data]

* Semi-supervised approach
  * Divided World into regions (Western Hemisphere (160°W to 20°W), Eastern Hemisphere-1 (20°W to 100°E), and Eastern Hemisphere-2 (100°E to 160°W))
  * Divided them into 14 Biomes 
  * Stratified samples based on NASA MCD12Q1 land cover for 2017 + others
]
.panel[.panel-name[Training]

* Expert group labeled approximately 4,000 image tiles  
* Non-expert - 20,000
* 409 image tiles were held back 
* minimum mapping unit of 50 × 50 m (5 × 5 pixels) used Labelbox
* label at least 70% of a tile within 20 to 60 minutes

* skill differential between the non-expert and expert groups
* linearly interpolating the distributions per-pixel from their one-hot encoding, weight on 0.2 experts and 0.3 non-experts

]
.panel[.panel-name[Pre-processing]

* Use SR for labelling BUT used TOA (level L1C) for the model as SR only from 2017
* Masked clouds and shadows
* Weights for each pixel (I think these are the probabilities for each pixel based on the user weights)
  * They work out the 
* Augmentations - rotation of image (rotate them) bands (band ratioing) to improve model
]

.panel[.panel-name[Normalisation]

* We first log-transform the raw reflectance values to equalize the long tail of highly reflective surfaces

* remap percentiles of the log-transformed values to points on a sigmoid function

* use these output values which reduce the value ranges
]

.panel[.panel-name[Classification]


* Fully Convolutional Neural Network (FCNN)
* Learns a mapping from estimated probabilities back to the input reflectances (synthesis model gradiet)
  * basically means it is re-learning from the output data "the backward" model
* Pass all normalized bands except B1, B8A, B9 and B10 after bilinear upscaling to ee.Model.predictImage.

* Runs automatically after each new image

* It looks blobby as the training data is 50x50m and also CNN*


]

.panel[.panel-name[CNN]


* Convolution Neural Network (ConvNet/CNN) form of **deep learning**

* Deep learning is a sub section of machine learning focused on neural networks with big datasets

* The potential issue here is with the convolution = a moving window filter (see next tab)

* This is the start of the CNN process

* Similar to texture, using a moving window.

Further reading: [A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)



]

.panel[.panel-name[CNN 2]


&lt;img src="img/conv.gif" width="100%" /&gt;
.small[Source:[Original from the British Library. Digitally enhanced by rawpixel.](https://www.rawpixel.com/image/571789/solar-generator-vintage-style)]

]

.panel[.panel-name[Accuracy]


* Accuracy is assessed through a **Confusion matrix** - see next slides
  * This is a common approach in classification
  
* However, they note that this might not be appropriate:
  * Different products
  * Live updates 

]

.panel[.panel-name[Example]

&lt;img src="img/dynamic_world.PNG" width="40%" style="display: block; margin: auto;" /&gt;

.small[Visual comparison of Dynamic World (DW) to other global and regional LULC datasets for validation tile locations in (A) Brazil (−11.437°, −61.460°), (B) Norway, (61.724°, 6.484°), and (C) the United States (39.973°, −123.441°). Datasets used for comparison include 300 m European Space Agency (ESA) Climate Change Initiative (CCI); 100 m Copernicus Global Land Service (CGLS) ProbaV Land Cover dataset; 10 m ESA Sentinel-2 Global Land Cover (S2GLC) Europe 2019; 30 m MapBiomas Brazil dataset; and 30 m USGS National Land Cover Dataset (NLCD). Each map chip represents a 5.1 km by 5.1 km area with corresponding true-color (RGB) Sentinel-2 image shown in the first column. All products have been standardized to the same legend used for DW. Note differences in resolution as well as differences in the spatial distribution and coverage of land use land cover classes. Source:[Brown et al. 2022](https://www.nature.com/articles/s41597-022-01307-4#code-availability)]
]

.panel[.panel-name[Notes]

* The training data is online and also used in the ESRI LULC 2020 map: https://doi.pangaea.de/10.1594/PANGAEA.933475?format=html#download

* The code is online - see the paper.
]
.panel[.panel-name[Radiant MLHub]

* At the same time (sadly) [Radiant MLHub](https://mlhub.earth/) launched the first open library dedicated to EO training for machine learning...

&lt;img src="img/landcovernet.png" width="45%" style="display: block; margin: auto;" /&gt;
.small[Source:[Radiant MLHub](https://mlhub.earth/datasets?search=landcovernet)]
]
]
---
class: inverse, center, middle


# Before we progress....thoughts on this?

--

### What was the data (SR, TOA)

### How was it trained

### What are the issues

### Do you think it's any good

---

class: inverse, center, middle

# Next up 

# Object based image analysis and sub pixel analysis

---

# Object based image analysis (OBIA)

.pull-left[

* Does a raster cell represent an object on the ground?

* Instead of considering cells we consider shapes based on the similarity (homogeneity) or difference (heterogeneity) of the cells = **superpixels**

* **SLIC** (Simple Linear Iterative Clustering) Algorithm for Superpixel generation is the [most common method](doi:10.1109/TPAMI.2012.120)
  * regular points on the image 
  * work out spatial distance (from point to centre of pixel) = **closeness to centre**
  * colour difference (RGB vs RGB to centre point) = **homogenity of colours** 



]

.pull-right[

&lt;img src="img/supercells.gif" width="40%" style="display: block; margin: auto;" /&gt;

.small[Supercells Source:[Nowosad 2021](https://jakubnowosad.com/ogh2021/#10)]
]

]

---

# Object based image analysis (OBIA) 2

.pull-left[

* Each iteration the centre moves- 4-10 is best (based on orignal paper)

* The values can change and the boarders move (like k-means?)

* Doesn't consider connectivity = very small cells

* Can enforce connectivity (remove small areas and merge them)

* `\(S\)` = distance between initial points

* `\(m\)` = compactness = balance between physical distance (larger value) and colour (spectral distance, then smaller `\(m\)`) 
]

.pull-right[

* Can only use Euclidean distance in SLIC

&lt;img src="img/SLIC.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Supercells Source:[Darshite Jain](https://darshita1405.medium.com/superpixels-and-slic-6b2d8a6e4f08)]
]

---

# Object based image analysis (OBIA) 3


.pull-left[

* **Supercells** package can use any distance measure (e.g. dissimilarity)

* `\(k\)` = number of super pixels

* `\(compactness\)` = impact of spatial (higher value) vs colour (lower value)

* `\(transform\)` = not on raw data, but to LAB colour space

* We can then take the [average values per object](https://jakubnowosad.com/ogh2021/#24) and classify them using methods we've seen

* Other metrics can also be computed - e.g. length to width ratio (see Jensen p.418)

]

.pull-right[
&lt;img src="img/supercells2.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Supercells Source:[Nowosad 2021](https://jakubnowosad.com/ogh2021/#10)]
]

---
# Object based image analysis (OBIA) 3


Note that there are many OBIA classifiers, they all do similar, but slightly different processes - see Jensen page 415

A more advanced package would be [**SegOptim**](https://segoptim.bitbucket.io/docs/) that can use algorithms from other software

&lt;img src="img/segOptim.jpg" width="55%" style="display: block; margin: auto;" /&gt;

.small[SegOptim Source:[João Gonçalves 2020](https://segoptim.bitbucket.io/docs/)]

---

# Sub pixel analysis 

If you have a pixel composed of a range of land cover types should it **be classified as one landcover** or should **we calculate the proportions ?**

&lt;img src="img/high_res_medium_res.PNG" width="40%" style="display: block; margin: auto;" /&gt;

.small[Comparison of true colour high spatial resolution data (a) (acquired from 14 March 2007) and Landsat surface reflectance (b) (acquired on 6 October 2007 [path 112]), highlighting the spatial detail captured by high-resolution imagery (c) and the same areas as observed by Landsat (d) for the subset East Beechboro used within this study Source:[MacLachlan et al. 2017](https://www.tandfonline.com/doi/pdf/10.1080/01431161.2017.1346403?needAccess=true&amp;)]

---

# Sub pixel analysis 

Termed (all the same): Sub pixel classification, Spectral Mixture Analysis (SMA), Linear spectral unmixing 

.pull-left[

* SMA determines the **proportion** or **abundance** of landcover per pixel

* the assumption that reflectance measured at each pixel is represented by the linear sum of endmembers weighted by the associated endmember fraction

* Typically we have a few endmembers that are **spectrally pure**
]

.pull-right[

&lt;img src="img/Perfect-decomposition-with-a-Linear-Spectral-Mixture-Model-LSMM-on-a-30-m-pixel-formed.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Machado and Small (2013) 2017](https://www.researchgate.net/figure/Perfect-decomposition-with-a-Linear-Spectral-Mixture-Model-LSMM-on-a-30-m-pixel-formed_fig6_259715697)]

]

In R we can [use MESMA from the package RStoolbox](https://jakob.schwalb-willmann.de/blog/spectral-unmixing-using-rstoolbox/) 
---
# Sub pixel analysis 2

.pull-left[

* Sum of end member reflectance * fraction contribution to best-fit mixed spectrum


`$$p_\lambda=\sum_{i=1}^{n} (p_{i\lambda} * f_i) + e_\lambda$$`
`\(p_\lambda\)` =  The pixel reflectance

`\(p_i\lambda\)` = reflectance of endmember `\(i\)`

`\(f_i\)` = fractional cover of end member `\(i\)`

`\(n\)` = number of endmembers

`\(e_\lambda\)` = model error

See, Jensen page 480 - following example taken from there
]

.pull-right[

&lt;img src="img/mixture.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Plaza et al. (2002)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1046852)]

]


---

# Sub pixel analysis 3

Not as complicated as it looks...here are some end members for bands 3 and 4

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Band &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Water &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Vegetation &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Soil &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 70 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 60 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

We take the [inverse matrix](https://www.mathsisfun.com/algebra/matrix-inverse.html) of them ...

`$$\begin{bmatrix}13 &amp; 22 &amp; 70\\
5 &amp; 80 &amp; 60\\
1 &amp; 1 &amp; 1
\end{bmatrix} \begin{bmatrix}-0.0053 &amp; -0.0127 &amp; 1.1322\\
-0.0145 &amp; 0.0150 &amp; 0.1137\\
0.0198 &amp; -0.0024 &amp; -0.2460
\end{bmatrix}$$`

---
# Sub pixel analysis 4


Then solve ... if our values for the pixel are **25** (band 3) and **57** (band 4)the rows of the first matrix are multiplied by the columns of the second one

`$$\begin{bmatrix}f_{water}\\
f_{veg}\\
f_{soil}
\end{bmatrix}\begin{bmatrix}-0.0053 &amp; -0.0127 &amp; 1.1322\\
-0.0145 &amp; 0.0150 &amp; 0.1137\\
0.0198 &amp; -0.0024 &amp; -0.2460
\end{bmatrix} \begin{bmatrix}25\\
57\\
1
\end{bmatrix}$$`
This looks like...(from [matrix  calculator](https://matrixcalc.org/en/))

`$$\left(\begin{matrix}
\frac{-53}{10000}*25+\frac{-127}{10000}*57+\frac{5661}{5000}*1 \\
\frac{-29}{2000}*25+\frac{3}{200}*57+\frac{1137}{10000}*1 \\
\frac{99}{5000}*25+\frac{-3}{1250}*57+\frac{-123}{500}*1
\end{matrix}\right)$$`
---
# Sub pixel analysis 5

And gives...
`$$\begin{bmatrix}0.27\\
0.61\\
0.11
\end{bmatrix}\begin{bmatrix}-0.0053 &amp; -0.0127 &amp; 1.1322\\
-0.0145 &amp; 0.0150 &amp; 0.1137\\
0.0198 &amp; -0.0024 &amp; -0.2460
\end{bmatrix} \begin{bmatrix}25\\
57\\
1
\end{bmatrix}$$`
This means that within this pixel we have:

* 27% water
* 61% vegetation
* 11% soil

---
# Sub pixel analysis 6

Issues / considerations:

.pull-left[

* Pixel purity ?

* Number of End members
  * simplify the process and use the **V-I-S model** in urban areas: Vegetation-Impervious surface-Soil (V-I-S) fractions

* Multiple endmember spectral analysis (MESMA)
  * Increase computation
  * or use a spectral library 
]

.pull-right[

&lt;img src="img/VIS.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Phinn et al. (2002) Monitoring the composition of urban environments based on the vegetation-impervious surface-soil (VIS) model by subpixel analysis techniques,](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1046852)]

]

---
# Accuracy assessment

After producing and output we need to assign an accuracy value to it (common to machine learning).

.pull-left[

In remote sensing we focus on:

* PA Producer accuracy (recall or true positive rate or sensitivity)
* UA User’s accuracy (consumer’s accuracy or precision or
positive predictive value
* OA the (overall) accuracy
]

.pull-right[
&lt;img src="img/matrix.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Barsi et al. 2018 Accuracy Dimensions in Remote Sensing](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf)]
]

---
# Accuracy assessment 2

.pull-left[

&lt;img src="img/true_positive.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[PICO](https://www.pico.net/kb/what-is-a-false-positive-rate/)]

* Although i think this image is **wrong**

**Where model is correct**

* True positive = model predicts positive class correctly

* True negative = model predicts negative class correctly 

** Where model is incorrect**

* False positive = model predicts positive, but it is negative

* False negative = model predicts negtaive, but it is positive

]


.pull-right[
&lt;img src="img/matrix.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Barsi et al. 2018 Accuracy Dimensions in Remote Sensing](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf)]
]

---


# Accuracy assessment 3

.pull-left[

* **producer’s accuracy** defined as the fraction of correctly classified pixels (TP) compared to ground truth data (TP+FN) `\(\frac{TP}{TP+FN}\)`

* **user’s accuracy** defined as the fraction of correctly classified pixels (TP) relative to all others classified as a particular land cover(TP+FP) `\(\frac{TP}{TP+FP}\)` - **FP** is different

* **overall accuracy** that represents the combined  fraction of correctly classified pixels (TP +TN) across all land cover types (TP+FP+FN+TN) `\(\frac{TP +TN}{TP+FP+FN+TN}\)`

]

.pull-right[

&lt;img src="img/matrix.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Barsi et al. 2018 Accuracy Dimensions in Remote Sensing](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf)]
]

---

# Accuracy assessment 4

Example

&lt;img src="img/example_matrix.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Brown et al. 2022 Dynamic World, Near real-time global 10m land use land cover mapping](https://www.nature.com/articles/s41597-022-01307-4.pdf)]
]
---

# Accuracy assessment 5

.pull-left[

* Errors of omission (100-producer's accuracy)
  * Landcover omitted from correct class
  * Type 1 error
  * Urban = 1/23, 4%
  * Urban producer = 22/23, 95.65%
   
* Errors of commission (100- user's accuracy)
  * Classified sites for incorrect classifications
  * Urban = 9/31, 29%
  * Urban user = 7/31, 22.58%

* Kappa coefficient


]

.pull-right[

&lt;img src="img/accuracy_2.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Earth Systems Science and Remote Sensing](https://medium.com/@wenzhao.li1989/accuracy-assessment-d164e492274b)]
]
---
# Accuracy assessment 6


Producer accuracy ... 

.pull-left[

&gt; i am pleased that 95.65 % of the urban area that was identified in the reference is urban in the classification 

User accuracy ... 

&gt; as a user i only find that only 22.58% of the time when i visit an urban area is it acutally urban

Overall accuracy is 77.89%

* is this acceptable for the user? 

* there is no single right choice for accuracy measurements

]

.pull-right[

&lt;img src="img/accuracy_2.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Earth Systems Science and Remote Sensing](https://medium.com/@wenzhao.li1989/accuracy-assessment-d164e492274b)]

* This can also be changed to a fuzzy matrix (e.g. Decidous forest classified as evergreen forest - see Jensen p.575)

]
---
# Accuracy assessment 8

### To Kappa or not to Kappa?

* Designed to express the accuracy of an image compared to the results by chance

* Ranges from 0 to 1

&gt; "Sadly the calls to abandon the use of the kappa coefficient in accuracy assessment seem to have fallen on deaf ears. It may be that the kappa coefficient is still widely used because it has become ingrained in practice and there may be a sense of obligation to use it"

.center[

`\(k=\frac{p_o - p_e}{1- p_e}\)`
]

`\(p_o\)` is the proportion of cases correctly classified (accuracy)

`\(p_e\)` expected cases correctly classified by chance (further equations in [Foody 2020](https://reader.elsevier.com/reader/sd/pii/S0034425719306509?token=47B253784FA5346F4A2E26B6DA796DBE71DC53A34AE76AAB1FFA43927EC021937C0C108A42154C4AE774083E4C7BD52F&amp;originRegion=eu-west-1&amp;originCreation=20220707132403)) or [Earth Systems Science and Remote Sensing](https://medium.com/@wenzhao.li1989/accuracy-assessment-d164e492274b)

.small[Source:[Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification. Foody 2020](https://reader.elsevier.com/reader/sd/pii/S0034425719306509?token=47B253784FA5346F4A2E26B6DA796DBE71DC53A34AE76AAB1FFA43927EC021937C0C108A42154C4AE774083E4C7BD52F&amp;originRegion=eu-west-1&amp;originCreation=20220707132403)]
---
# Kappa issues 


.pull-left[

* What is a good value?

&lt;img src="img/Kappa_issue1.png" width="100%" style="display: block; margin: auto;" /&gt;


]

.pull-right[

* How Kappa values can we have for different levels of accuracy (on x axis )


&lt;img src="img/Kappa_issue2.png" width="100%" style="display: block; margin: auto;" /&gt;

]

.small[Source:[Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification. Foody 2020](https://reader.elsevier.com/reader/sd/pii/S0034425719306509?token=47B253784FA5346F4A2E26B6DA796DBE71DC53A34AE76AAB1FFA43927EC021937C0C108A42154C4AE774083E4C7BD52F&amp;originRegion=eu-west-1&amp;originCreation=20220707132403)]
---
class: inverse, center, middle

# Have i used Kappa? 

--

### See Jensen page 570

---


class: inverse, center, middle

# In remote sensing this is typically where we'd stop...but not necessarily in machine learning 

---

class: inverse, center, middle

# A brief overview...

---

# Beyond remote sensing 


Beyond **traditional** remote sensing accuracy assessment...

.pull-left[

Problem with recall (Producer accuracy) vs Precision (User accuracy)

False positives (Producer) or false negatives (User) more important?


* model with high recall (Producer accuracy) = true positives but some false positives (predicted urban but land cover that isn't urban)

* Model with high precision (User’s accuracy) = actual urban but predicted other landcover

* See [MLU-explain](https://mlu-explain.github.io/precision-recall/) for an interactive example and next slide...
]

.pull-right[

&lt;img src="img/true_positive.png" width="80%" style="display: block; margin: auto;" /&gt;

.small[Source:[PICO](https://www.pico.net/kb/what-is-a-false-positive-rate/)]


&lt;img src="img/matrix.PNG" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[Barsi et al. 2018 Accuracy Dimensions in Remote Sensing](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf)]
]


---
# Our data is not balanced....

We can't have both a high high producer accuracy (recall) **and** a high user's accuracy (precision)

.pull-left[

* **user’s accuracy** (precision) ratio of correctly predicted positive classes (TP) to all items predicted to be positive (TP+FP)
 `\(\frac{TP}{TP+FP}\)` 1/1=100% or 8/19=42%
 
&gt; how precise the model is at positive predictions 


]

.pull-right[


&lt;img src="img/precision.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[MLU-explain](https://mlu-explain.github.io/precision-recall/)]




]

---

# Our data is not balanced....2

We can't have both a high high producer accuracy (recall) **and** a high user's accuracy (precision)

.pull-left[


* **producer’s accuracy** (recall) the ratio of correctly predicted positive classes (TP) to all items that are actually positive (TP+FN) `\(\frac{TP}{TP+FN}\)` 1/8=13% or 8/8=100%

&gt; how many positive points are correct


]

.pull-right[


&lt;img src="img/recall.png" width="80%" style="display: block; margin: auto;" /&gt;

.small[Source:[MLU-explain](https://mlu-explain.github.io/precision-recall/)]


]


---

# F1 🚗

.pull-left[

The F1-Score (or F Measure) combines both recall (Producer accuracy) and Precision (User accuracy):


* `\(F1 = \frac{2 * Precision * Recall}{Precision+Recall}\)`

Which equals....

* `\(F1 = \frac{TP}{TP + \frac{1}{2}*(FP+FN)}\)`

* Value from 0 to 1, where 1 is better performance

.small[Source:[MLU-EXPLAIN](https://mlu-explain.github.io/precision-recall/)]
]


.pull-right[
&lt;img src="img/matrix.PNG" width="80%" style="display: block; margin: auto;" /&gt;

.small[Source:[Barsi et al. 2018 Accuracy Dimensions in Remote Sensing](https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf)]
]

---
# F1 🚗 issues

.pull-left[
* No True Negatives (TN) in the equation
  * negative categories that are correctly classified as negative

* Are precision and recall equally important ? 
  * precision (producer): how many positive points are correct
  
  * recall (user): how precise the model is at positive predictions 

* What if our data is very unbalanced ? 
  * More negatives than positives?
]

.pull-right[

&lt;img src="img/trade_off.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[MLU-EXPLAIN](https://mlu-explain.github.io/precision-recall/)]
]
---
# Receiver Operating Characteristic Curve

Receiver Operating Characteristic Curve (the ROC Curve)

.pull-left[

* Receiver Operating Characteristic Curve (the ROC Curve)

* Originates from WW2, USA wanted to minimize noise from radar to identity (true positives) and not miss aircraft...minimizing false positives (clouds)

* **Changing the threshold value of classifier** will change the True Positive rate

  * probability that a positive sample is correctly predicted in the positive class. E.g., the percentage of radar signals predicted to be airplanes that actually are airplanes.

]

.pull-right[

* False positive rate: The probability that a negative sample is incorrectly predicted in the positive class

* Maximise true positives (1) and minimise false positives (0)

&lt;img src="img/ROC_planes.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[MLU-EXPLAIN](https://mlu-explain.github.io/roc-auc/)]

]

---
# Receiver Operating Characteristic Curve

&gt; Goal: Maximise true positives and minimise false positives...

&lt;img src="img/ROC.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[MLU-EXPLAIN](https://mlu-explain.github.io/roc-auc/)]


---
# Area Under the ROC Curve

Area Under the ROC Curve (AUC, or AUROC)

.pull-left[

* Simply the area under the curve 

* Compare models easily (no need to look at the ROC curve)

* Perfect value will be 1, random will be 0.5

&gt; "The AUC is the probability that the model will rank a randomly chosen positive example more highly than a randomly chosen negative example."...

e.g. model always give positive from true negative (so always wrong) = AUC 0
]

.pull-right[

&lt;img src="img/AUC.png" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source:[MLU-EXPLAIN](https://mlu-explain.github.io/roc-auc/)]

]

---
class: inverse, center, middle

# How to we get test data for the accuracy assessment?

---

# Remote sensing approach (sometimes) 


.pull-left[

Same process for all:

  * class definition
  * pre-processing
  * training
  * pixel assignment
  * accuracy assessment

**Guidelines**

* Collect training data - suggested as around 250 pixels per class (Foody and Mather, 2006)

* Simply go and collect (or use Google Earth) ground truth data - 50 per class (Congalton, 2001).

* Produce an error matrix

] 
 
  
.pull-right[
&lt;img src="img/supervised-diagram.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[GIS Geography](https://gisgeography.com/supervised-unsupervised-classification-arcgis/)
]

* You would need to consider a **sampling strategy**

  * Random sampling
  * Systematic sampling
  * Stratified sampling
  * Jensen p. 565
]


---

class: inverse, center, middle

# Problems? 

--

### Chapter 13 in Jensen (p.557) cover accuracy assessment...but not the following


---

# Good approach - train and test split 


* This is simply holding back a % of the original data used to train the model to then test it at the end

* See the [validation section (10.6.7)](https://andrewmaclachlan.github.io/CASA0005repo_20192020/advanced-r-maup-and-more-regression.html) for an example in linear regression


&lt;img src="img/train_test_split.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Michael Galarnyk](https://towardsdatascience.com/understanding-train-test-split-scikit-learn-python-ea676d5e3d1)
]



---

class: inverse, center, middle


# BUT..."Spatial autocorrelation between training and test sets"

--

## Remember spatial autocorrelation?

--

## A measure of similarity between nearby data...

---

# Spatial dependence....


Karasiak et al. 2022, [Spatial dependence between training and test sets: another pitfall of classification accuracy assessment in remote sensing](https://link.springer.com/article/10.1007/s10994-021-05972-1)

&lt;img src="img/spatial_dependance.png" width="55%" style="display: block; margin: auto;" /&gt;
.small[Average overall accuracy based on the RF classifier for each cross-validation strategy (k-fold CV, LOO CV, SLOO CV) at pixel and object levels. Models were fitted with reference samples of Herault-34 and repeated 10 times (i.e. the y-axis provides the average OA value ± standard deviation). The premature stopping of the pixel-based LOO and SLOO CV approaches was due to excessive computational time Source:[Karasiak et al. 2022](https://towardsdatascience.com/understanding-train-test-split-scikit-learn-python-ea676d5e3d1)
]

(1) a k-fold cross-validation (k-fold-CV) based on random splitting
(2) a non-spatial leave-one-out cross-validation (LOO CV) 
(3) a spatial leave-one-out cross-validation (SLOO CV) using a distance-based buffer relying on Moran’s I statistics.

---

class: inverse, center, middle


# Why do the pixels perform better than the objects?

---

# Best approach - cross validation

.pull-left[

Really classification of imagery is a machine learning task...

..So why can't we apply the same methods?

Perhaps as it is meant to be iterative...

...e.g. the classifier underpredicts urban then you can go and adjust the training data...

We might take the mean accuracy from the cross validation.

Leave one out cross validation is an extreme version where the folds (often 10) equals the number of samples in the data minus 1...next slide...

]

.pull-right[
&lt;img src="img/cross.jpg" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)
]
]
---
# Leave one out cross validation

* An extreme version of cross validation

* Uses all the training data except 1

* Repeats though all of it

&lt;img src="img/leave_one_out_CV.png" width="75%" style="display: block; margin: auto;" /&gt;
.small[Source:[Rahil Shaikh](https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85)
]
---
# Best approach - cross validation 2

Waldo Tobler's first Law of Geography...

&gt; "everything is related to everything else, but near things are more related than distant things."

* Are training and testing points too close in geographic space?

* How can we deal with taking a sample of training data for testing when they are possibly from the same polygon of training data...

&gt; ‘Training’ observations near the ‘test’ observations can provide a kind of ‘sneak preview’: information that should be unavailable to the training dataset. 

.small[Source:[Lovelace et al. 2022](https://geocompr.robinlovelace.net/spatial-cv.html#intro-cv)]

---



&lt;img src="img/more_related.jpg" width="60%" style="display: block; margin: auto;" /&gt;
.small[Source:[Spatial is Special](https://www.e-education.psu.edu/maps/l2_p2.html)]


---

class: inverse, center, middle


# Spatial cross validation 


---

#  Spatial cross validation 2

.pull-left[

* spatially partition the folded data, folds are from cross validation

* disjoint (no common boundary) using k -means clustering (number of points and a distance)

* same as cross validation but with clustering to the folds...

* stops our training data and testing data being near each other...
  
  &gt; in other words this makes sure all the points (or pixels) we train the model with a far away from the points (or pixels) we test the model with

]

.pull-right[
&lt;img src="img/13_partitioning.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Spatial visualization of selected test and training observations for cross-validation of one repetition. Random (upper row) and spatial partitioning (lower row). Source:[Lovelace et al. 2022](https://geocompr.robinlovelace.net/spatial-cv.html)
]
]

---

#  Spatial cross validation 3

Lovelace et al. (2022) use a Support Vector Machine classifier that requires hyperparameters (set before the classification)

Standard SVM then the classifier will try to **overfit** = perfect for the current data but useless for anything else...

Cortes and Vapnik - **soft margin**, permit classifications = controlled with **C**

.pull-left[

&lt;img src="img/overfit.png" width="60%" style="display: block; margin: auto;" /&gt;
.small[Source:[Soner Yildirim](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)
]
]

.pull-right[

&lt;img src="img/soft_SVM.png" width="60%" style="display: block; margin: auto;" /&gt;
.small[Source:[Soner Yildirim](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)
]

]
* **C** = adds penalty (proportional to distance from decision line) for each classified point. Small = image on right, large = image on left

.small[Source:[Soner Yildirim](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)
]

---

#  Spatial cross validation 4

Lovelace et al. (2022) use a Support Vector Machine classifier that requires hyperparameters (set before the classification)


* **Gamma (or also called Sigma)** = controls the influence of a training point within the classified data
  * low = big radius and many points in same group
  * high = low radius and many groups
  
  
.pull-left[

&lt;img src="img/low_gamma.png" width="60%" style="display: block; margin: auto;" /&gt;
.small[Source:[Soner Yildirim](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)
]
]

.pull-right[

&lt;img src="img/high_gamma.png" width="60%" style="display: block; margin: auto;" /&gt;
.small[Source:[Soner Yildirim](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)
]
]

.small[Source:[Soner Yildirim](https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167)
]
---

#  Spatial cross validation 5

* **Performance level** each spatial fold (taken from our first k-means cross validation fold division). = Top row below

* **Tuning level** each fold (outer) is then divided into 5 again (inner fold).= Bottom row below

*  **Performance estimation**  Use the 50 randomly selected hyperparameters in each of these inner subfolds, i.e., fit 250 models with random **C** and **Gamma** use the best values to outer fold = **AUROC**


&lt;img src="img/13_cv.png" width="75%" style="display: block; margin: auto;" /&gt;
.small[Schematic of hyperparameter tuning and performance estimation levels in CV. (Figure was taken from Schratz et al. (2019).. Source:[Lovelace et al. 2022](https://geocompr.robinlovelace.net/spatial-cv.html)
]

---
class: inverse, center, middle

## See the figure on the previous sldie..."Using the same data for the performance assessment and the tuning would potentially lead to overoptimistic results"

## Here tuning of parameters is made on a different subset of the data within each fold...

---
#  Spatial cross validation 6


.pull-left[

**Here we have..**.

* 1 outer fold has 5 inner folds with 50 models = 250 models for **C** and **Gamma**

It's 5 (outer fold)

* Each repetition =  1,250 (250 * 5) 

It's 100 times repeated (5 fold cross validation)

* 125,500 models for best hyperparameters

]

.pull-right[

**So .... what**

&lt;img src="img/boxplot-cv-1.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Boxplot showing the difference in GLM AUROC values on spatial and conventional 100-repeated 5-fold cross-validation. Source:[Lovelace et al. 2022](https://geocompr.robinlovelace.net/spatial-cv.html)
]

]

---

class: inverse, center, middle

# Question: What happens if a classificaiton model doesn't consider spatial autocorrelation ?

--
## The model will have better accuracy that it actually does

---


class: inverse, center, middle

# Question: What methods can we use to deal with it?


---

# Summary 

* How should we consider / process EO data, **objects, pixels, mixels (mixed pixel), mixed objects ?**

* What **data should we use assess the accuracy** of our classification models 
  * New dataset to test the output with
  * Train / split the training data
  * Cross validation 

* When we have a test dataset **how do we assess the accuracy** 
  * Error matrix 
  * Kappa 
  
* When training and testing our classification models do we need to **consider spatial autocorrelation?** Do the following help 

  * Object based image analysis 
  * Spatial cross validation



---

# Reading

[Land Use Cover Datasets and Validation Tools](https://link.springer.com/content/pdf/10.1007/978-3-030-90998-7.pdf)

https://doodles.mountainmath.ca/blog/2019/10/07/spatial-autocorrelation-co/

https://geocompr.robinlovelace.net/spatial-cv.html

https://link.springer.com/article/10.1007/s10994-021-05972-1

https://geocompr.robinlovelace.net/spatial-cv.html

https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/#:~:text=fold%20Cross%2DValidation-,Leave%2Done%2Dout%20cross%2Dvalidation%2C%20or%20LOOCV%2C,has%20the%20maximum%20computational%20cost.

https://machinelearningmastery.com/k-fold-cross-validation/

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
